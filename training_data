import os
#import torch
import torchvision
#from torch import nn
#from torch.utils.data import DataLoader
#from torchvision import datasets
#from torchvision.transforms import ToTensor
#from torch.utils.data import Dataset
from torchvision import datasets
#import time
#import pandas as pd
#from torchvision import transforms as pth_transforms
from sklearn.model_selection import train_test_split
import random
import shutil

def download_dataset():
    os.system("start /wait cmd /c \"curl \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\" --output training-datasets\\voc12.tar\"")
    os.chdir("training-datasets")
    os.system("start /wait cmd /c \"tar -xf voc12.tar\"")
    os.chdir("..")
    print(os.path.abspath(os.curdir))

def split_dataset(data_dir, split_ratio=0.8):
    # Create directories for train and test sets
    train_dir = os.path.join(data_dir, 'train')
    test_dir = os.path.join(data_dir, 'test')
    os.makedirs(train_dir, exist_ok=True)
    os.makedirs(test_dir, exist_ok=True)

    image_filenames = [filename for filename in os.listdir(os.path.join(data_dir, 'JPEGImages')) if filename.endswith('.jpg')]

    # shuffle liste af filenames
    random.shuffle(image_filenames)

    # Split the list of image filenames into train and test sets
    num_train = int(len(image_filenames) * split_ratio)
    train_image_filenames = image_filenames[:num_train]
    test_image_filenames = image_filenames[num_train:]

    for filename in train_image_filenames:
        shutil.copy2(os.path.join(data_dir, 'JPEGImages', filename), os.path.join(train_dir, filename))

    for filename in test_image_filenames:
        shutil.copy2(os.path.join(data_dir, 'JPEGImages', filename), os.path.join(test_dir, filename))

    # Split annotations files into train and test directories
    for filename in train_image_filenames:
        shutil.copy2(os.path.join(data_dir, 'Annotations', filename.replace('.jpg', '.xml')), os.path.join(train_dir, filename.replace('.jpg', '.xml')))
    for filename in test_image_filenames:
        shutil.copy2(os.path.join(data_dir, 'Annotations', filename.replace('.jpg', '.xml')), os.path.join(test_dir, filename.replace('.jpg', '.xml')))

    print("Number of training images:", len(train_image_filenames))
    print("Number of test images:", len(test_image_filenames))

if not os.path.exists('training-datasets\VOCdevkit'):
    download_dataset()

if not os.path.exists('training-datasets/VOCdevkit/VOC2012/test'):
    split_dataset("training-datasets/VOCdevkit/VOC2012")

'''
transform = pth_transforms.Compose(
    [
        pth_transforms.ToTensor(),
        pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
    ]
)

training_data = datasets.VOCDetection(
    root = "training-datasets",
    year = "2012",
    image_set = "train",
    transform = transform,
    target_transform = torchvision.transforms.Resize((100, 100)),
    download = True,
)

test_data = datasets.VOCDetection(
    root = "test-datasets",
    year = "2012",
    image_set = "val",
    transform = transform,
    target_transform = torchvision.transforms.Resize((100, 100)),
    download = True,
)

train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)

start = time.time()
device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
print(f"Using {device} device")

# Define model
class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10)
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

model = NeuralNetwork().to(device)
print(model)

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)

def train(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    model.train()
    for batch, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device)

        # Compute prediction error
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            loss, current = loss.item(), (batch + 1) * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

def test(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    model.eval()
    test_loss, correct = 0, 0
    with torch.no_grad():
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()
    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")

epochs = 20
for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train(train_dataloader, model, loss_fn, optimizer)
    test(test_dataloader, model, loss_fn)
    end = time.time()
    print(f'Total time elapsed: {round((end-start)/60, 2)} minutes')
print("Done!")

'''
